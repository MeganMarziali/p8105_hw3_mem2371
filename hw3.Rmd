---
title: "Homework 3"
author: Megan Marziali
output: github_document
---

## Beginning infrastructure

This code chunk calls any relevant libraries and setting options.

```{r, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1: Instacart

#### Loading instacart dataset

The following code chunk loads the Instacart dataset

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by users of instacart. There are user and order variables -- user_id, order id, order day, and order hour. There are also item variables -- name, aisle, department and some numeric codes.

#### Answering specific questions

The following code is to check how many aisles there are which are ordered from.

```{r aisles, message = FALSE, results = FALSE}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Now, we need to make a plot of the number of orders purchased per aisle.

```{r plot_aisles}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Aisle order plot",
    x = "Aisle names",
    y = "Number of orders",
    caption = "Data from instacart."
    )
```

The next step is to make a table showing the most popular items.

```{r pop_items}
instacart %>% 
  filter(aisle %in% c(
    "baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

The next step is to make a table of the mean hour at which specific items are ordered.

```{r, message = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2: Accelerometer

The following code is to load in the accelerometer data.

```{r, message=FALSE}
accel_df = 
  read_csv("./problem2/accel_data.csv",
           na = "") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    day_type = recode(
      day, 
      Monday = "Weekday",
      Tuesday = "Weekday",
      Wednesday = "Weekday",
      Thursday = "Weekday",
      Friday = "Weekday", 
      Saturday = "Weekend",
      Sunday = "Weekend"),
    day = factor(day),
    day = forcats::fct_relevel(day, c("Monday",
                                      "Tuesday",
                                      "Wednesday",
                                      "Thursday",
                                      "Friday",
                                      "Saturday",
                                      "Sunday")),
    minute = as.integer(minute)
  )
```

This dataset includes accelerometer data collected from a 65 year old patient. Observations correspond to activity counts over the course of 24 hours. The variables included are `r names(accel_df)`. There are a total of `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. Activity counts were taken every minute of the day, for a total of `r max(pull(accel_df, minute))` minutes. Measurements were taken every `r levels(pull(accel_df, day))`, for a total of `r max(pull(accel_df, week))` weeks and `r max(pull(accel_df, day_id))` days.

The value of the accelerometer readings as per the activity_count variable ranges from `r min(pull(accel_df, activity_count))` to `r max(pull(accel_df, activity_count))`. The mean of the accelerometer readings is `r mean(pull(accel_df, activity_count), n = 2)`, with a standard deviation of `r sd(pull(accel_df, activity_count), n = 2)`. The median is `r median(pull(accel_df, activity_count), n = 2)`, with an IQR of `r IQR(pull(accel_df, activity_count), n = 2)`. 

### Aggregating data

The following code makes a table, aggregated by total activity per day.

```{r, message = FALSE}
accel_df %>% 
  group_by(day, day_id) %>% 
  summarize(
    total_activity = sum(activity_count)
  ) %>% 
  pivot_wider(
    names_from = "day",
    values_from = "total_activity"
  ) %>% 
  knitr::kable(digits = 1)
```

It seems that the patient is the least active on Saturdays, particularly in the last two weeks. The patient seems the most active on Fridays, and potentially on Mondays. Certain days seem more consistent related to activity level than others (such as Tuesdays, Wednesdays and Thursdays).

```{r, message = FALSE}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Activity count over time of day",
    x = "Minutes",
    y = "Activity",
    caption = "Data from patient accelerometer."
    )
```

It seems that the patient is active more during the day on Sundays, and more active in the evening on Fridays. The time at which the patient is most active depends on the day. The patient also seems to be more active during the week than on the weekend and more active at certain points in the day. There is a peak on Sundays around minute 500, and on Fridays at around minute 1250.

## Problem 3: NY NOAA

The following code loads in the NY NOAA dataset.

```{r}
data("ny_noaa")
```

#### Cleaning and tidying

The following code cleans and tidies the data.

```{r, message = FALSE}
noaa_clean = 
  ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(
    date,
    sep = "-",
    into = c("year", "month", "day")
  ) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    prcp = prcp/10,
    tmax = as.integer(tmax)/10,
    tmin = as.integer(tmin)/10
  )
```

The NOAA dataset is from the National Climactic Data Center, and includes variables `r names(noaa_clean)`. There are `r ncol(noaa_clean)` columns and `r nrow(noaa_clean)` rows in this dataset. Observations relate to climate measures of various weather stations.

The time period of this data ranges from `r min(pull(noaa_clean, year))` to `r max(pull(noaa_clean, year))`. Ranges of climate variables are from: `r min(pull(noaa_clean, prcp), na.rm = TRUE)` to `r max(pull(noaa_clean, prcp), na.rm = TRUE)` for precipitation (mm); `r min(pull(noaa_clean, snow), na.rm = TRUE)` to `r max(pull(noaa_clean, snow), na.rm = TRUE)` for snowfall (mm); `r min(pull(noaa_clean, snwd), na.rm = TRUE)` to `r max(pull(noaa_clean, snwd), na.rm = TRUE)` for snow depth (mm);`r min(pull(noaa_clean, tmin), na.rm = TRUE)` to `r max(pull(noaa_clean, tmin), na.rm = TRUE)` for minimum temperatures (C); and, `r min(pull(noaa_clean, tmax), na.rm = TRUE)` to `r max(pull(noaa_clean, tmax), na.rm = TRUE)` for maximum temperatures (C).

There are `r sum(is.na(noaa_clean))` total missing observations, which is a large amount of missingness for this dataset.

#### Common snowfall

The following code assesses the most commonly observed values for snowfall.

```{r, message = FALSE, results = FALSE}
noaa_clean %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most common observed value for snowfall is 0 mm of snowfall. This could suggest either seasonality of snowfall, or that some weather stations are located in areas that are less likely to have snowfall compared to others.

The second most common value is "NA", which is expected given the large amount of missingness observed in the dataset.

#### Two-panel max temperature month plot

The following code makes a two-panel plot showing the average max temperature in January and July.

```{r message = FALSE, warning = FALSE}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name)

noaa_month = 
  left_join(noaa_clean, month_df, by = "month") %>% 
  select(-month)

noaa_month %>% 
  group_by(id, year, month_name) %>% 
  summarize(
    average = mean(tmax)
  ) %>% 
  filter(month_name %in% c("January", "July")) %>% 
  ggplot(aes(x = year, y = average)) +
  geom_point(alpha = 0.5, size = 0.5) +
  facet_grid(. ~ month_name) +
  labs(
    title = "Average maximum temperature over time for January and July",
    y = "Average maximum temperature (C)",
    caption = "Each dot indicates a weather station. Data from NOAA."
  )
```

In general, the average maximum temperatures in January are lower than they are in July. It seems that there is one discernable outlier in January in 1981, where the temperature is very cold. There seem to be more outliers in July than in January. July outliers suggest abnormally cold temperatures.

#### Two-panel tmax/tmin and snowfall distribution plot

Making a two-panel plot showing (i) tmax and tmin and (ii) plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r message = FALSE, warning = FALSE}
tmax_tmin_plot = 
  noaa_clean %>% 
  select(tmax, tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  theme(legend.position = "none") +
  labs(
    x = "Maximum temperature (C)",
    y = "Minimum temperature (C)",
    title = "Maximum versus minimum temperature (C)"
  )

snow_dens_plot = 
  noaa_clean %>% 
  filter(
    snow < 100,
    snow > 0
  ) %>% 
  ggplot(aes(group = year, x = year, y = snow)) +
  geom_boxplot() +
  theme(legend.position = "none") +
  labs(
    x = "Year",
    y = "Snowfall (mm)",
    title = "Distribution of snowfall by year"
  )

tmax_tmin_plot / snow_dens_plot
```

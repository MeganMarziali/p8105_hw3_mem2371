---
title: "Homework 3"
author: Megan Marziali
output: github_document
---

## Beginning infrastructure

This code chunk calls any relevant libraries and setting options.

```{r, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1: Instacart

#### Loading instacart dataset

The following code chunk loads the Instacart dataset

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by users of instacart. There are user and order variables -- user_id, order id, order day, and order hour. There are also item variables -- name, aisle, department and some numeric codes.

#### Answering specific questions

The following code is to check how many aisles there are which are ordered from.

```{r aisles, message = FALSE, results = FALSE}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Now, we need to make a plot of the number of orders purchased per aisle.

```{r plot_aisles}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Aisle order plot",
    x = "Aisle names",
    y = "Number of orders",
    caption = "Data from instacart."
    )
```

The next step is to make a table showing the most popular items.

```{r pop_items}
instacart %>% 
  filter(aisle %in% c(
    "baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

The next step is to make a table of the mean hour at which specific items are ordered.

```{r, message = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2: Accelerometer

The following code is to load in the accelerometer data.

```{r, message=FALSE}
accel_df = 
  read_csv("./problem2/accel_data.csv",
           na = "") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    day_type = recode(
      day, 
      Monday = "Weekday",
      Tuesday = "Weekday",
      Wednesday = "Weekday",
      Thursday = "Weekday",
      Friday = "Weekday", 
      Saturday = "Weekend",
      Sunday = "Weekend"),
    day = factor(day),
    day = forcats::fct_relevel(day, c("Monday",
                                      "Tuesday",
                                      "Wednesday",
                                      "Thursday",
                                      "Friday",
                                      "Saturday",
                                      "Sunday")),
    minute = as.integer(minute)
  )
```

This dataset includes accelerometer data collected from a 65 year old patient. Observations correspond to activity counts over the course of 24 hours. The variables included are `r names(accel_df)`. There are a total of `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. Activity counts were taken every minute of the day, for a total of `r max(pull(accel_df, minute))` minutes. Measurements were taken every `r levels(pull(accel_df, day))`, for a total of `r max(pull(accel_df, week))` weeks and `r max(pull(accel_df, day_id))` days.

The value of the accelerometer readings as per the activity_count variable ranges from `r min(pull(accel_df, activity_count))` to `r max(pull(accel_df, activity_count))`. The mean of the accelerometer readings is `r mean(pull(accel_df, activity_count), n = 2)`, with a standard deviation of `r sd(pull(accel_df, activity_count), n = 2)`. The median is `r median(pull(accel_df, activity_count), n = 2)`, with an IQR of `r IQR(pull(accel_df, activity_count), n = 2)`. 

### Aggregating data

The following code makes a table, aggregated by total activity per day.

```{r, message = FALSE}
accel_df %>% 
  group_by(day_id, day) %>% 
  summarize(
    total_activity = sum(activity_count)
  ) %>% 
  arrange(desc(total_activity)) %>% 
  knitr::kable(digits = 1)
```

It seems that the patient is the least active on Saturdays, and the most active on Fridays.

```{r, message = FALSE}
accel_df %>% 
  group_by(day_id, day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_line(alpha = 0.5) +
  geom_smooth()
```

It seems that the patient is active more during the day on Sundays, and more active in the evening on Fridays. The time at which the patient is most active depends on the day.

## Problem 3: NY NOAA

The following code loads in the NY NOAA dataset.

```{r}
data("ny_noaa")
```

The following code cleans and tidies the data.

```{r, message = FALSE}
noaa_clean = 
  ny_noaa %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  separate(
    date,
    sep = "-",
    into = c("year", "month", "day")
  ) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    prcp = prcp/10,
    tmax = as.integer(tmax)/10,
    tmin = as.integer(tmin)/10
  )
```

The following code assesses the most commonly observed values for snowfall.

```{r, message = FALSE}
noaa_clean %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most common observed value for snowfall is 0 mm of snowfall. The second most common value for snowfall is 25mm. This could suggest either seasonality of snowfall, or that some weather stations are located in areas that are less likely to have snowfall compared to others.

The following code makes a two-panel plot showing the average max temperature in January and July.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name)

noaa_month = 
  left_join(noaa_clean, month_df, by = "month") %>% 
  select(-month)

noaa_month %>% 
  group_by(id, year, month_name) %>% 
  summarize(
    average = mean(tmax)
  ) %>% 
  filter(month_name %in% c("January", "July")) %>% 
  ggplot(aes(x = year, y = average)) +
  geom_point(alpha = 0.5, size = 0.5) +
  facet_grid(. ~ month_name)
```

On average, the maximum temperatures in January are lower than they are in July. It seems that there are some discernible outliers, where the max temperature in January is much higher than the average. There are also certain outliers that are much colder in January. Similarly,there is one outlier which is much colder in July than the rest of the temperatures.

Making a two-panel plot showing (i) tmax and tmin and (ii) plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r}
tmax_tmin_plot = 
  noaa_clean %>% 
  select(tmax, tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  theme(legend.position = "none")

snow_dens_plot = 
  noaa_clean %>% 
  filter(
    snow < 100,
    snow > 0
  ) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_smooth() +
  theme(legend.position = "none")

tmax_tmin_plot + snow_dens_plot
```
